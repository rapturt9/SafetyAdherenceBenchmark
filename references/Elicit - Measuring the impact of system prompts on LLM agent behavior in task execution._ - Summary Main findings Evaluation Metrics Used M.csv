Title,Authors,DOI,DOI link,Venue,Citation count,Year,Abstract summary,Summary,Main findings,Evaluation Metrics Used,"Supporting quotes for ""Summary""","Supporting  tables for ""Summary""","Reasoning for ""Summary""","Supporting quotes for ""Main findings""","Supporting  tables for ""Main findings""","Reasoning for ""Main findings""","Supporting quotes for ""Evaluation Metrics Used""","Supporting  tables for ""Evaluation Metrics Used""","Reasoning for ""Evaluation Metrics Used"""
TraitsPrompt: Does Personality Traits Influence the Performance of a Large Language Model?,"Qingmeng Zhu, Tianxing Lan, Xiaoguang Xue, Zhipeng Yu, Hao He",10.1109/CSCWD61410.2024.10580436,https://doi.org/10.1109/CSCWD61410.2024.10580436,International Conference on Computer Supported Cooperative Work in Design,0,2024,Incorporating personality traits in prompts can enhance LLM performance on specialized tasks.,The paper investigates the impact of personality traits on the performance of large language models by using prompts based on the Big Five personality traits and comparing the model's performance on those tasks to a baseline.,"- The LLM performed better on tasks aligned with the Big Five personality traits, with performance enhancements ranging from 0.6% to 3.6%.
- Incorporating personality traits into prompts when querying the LLM can be an effective strategy for optimal execution of specific professional tasks.","The evaluation metrics used in this paper were performance enhancements ranging from 0.6% to 3.6% compared to a baseline, when incorporating personality traits into prompts for the LLM.","  - ""This paper delves into the impact of personality traits on model performance. Specifically, we employ the Big Five Personality Theory for model analysis, guiding questions in diverse datasets using prompts constructed around specific personality traits to study LLM outputs.""
  - ""In comparison to the baseline test questions, tasks aligned with the Big Five personality traits manifested superior performance, with enhancements ranging from 0.6% to 3.6%, mirroring human behavior closely.""
  - ""This underscores that for optimal execution of specific professional tasks, incorporating corresponding personality traits in prompts can be an effective strategy when querying the LLM.""",,"
The summary of this paper is that it investigates the impact of personality traits on the performance of large language models (LLMs) by using prompts based on the Big Five personality traits and comparing the model's performance on those tasks to a baseline.
""This paper delves into the impact of personality traits on model performance. Specifically, we employ the Big Five Personality Theory for model analysis, guiding questions in diverse datasets using prompts constructed around specific personality traits to study LLM outputs.""
""In comparison to the baseline test questions, tasks aligned with the Big Five personality traits manifested superior performance, with enhancements ranging from 0.6% to 3.6%, mirroring human behavior closely.""
""This underscores that for optimal execution of specific professional tasks, incorporating corresponding personality traits in prompts can be an effective strategy when querying the LLM.""
The paper examines how using prompts based on personality traits can improve the performance of LLMs on specialized tasks, suggesting that incorporating personality traits into prompts can be an effective strategy.
","  - ""In comparison to the baseline test questions, tasks aligned with the Big Five personality traits manifested superior performance, with enhancements ranging from 0.6% to 3.6%, mirroring human behavior closely.""
  - ""This underscores that for optimal execution of specific professional tasks, incorporating corresponding personality traits in prompts can be an effective strategy when querying the LLM.""",,"
The main findings of this study, as summarized in the abstract, are:

""In comparison to the baseline test questions, tasks aligned with the Big Five personality traits manifested superior performance, with enhancements ranging from 0.6% to 3.6%, mirroring human behavior closely.""
This quote indicates that the LLM performed better on tasks that were aligned with specific personality traits from the Big Five personality theory, compared to baseline tasks.

""This underscores that for optimal execution of specific professional tasks, incorporating corresponding personality traits in prompts can be an effective strategy when querying the LLM.""
This quote suggests that incorporating personality traits into prompts when querying the LLM can lead to improved performance on specific professional tasks.

","  - ""In comparison to the baseline test questions, tasks aligned with the Big Five personality traits manifested superior performance, with enhancements ranging from 0.6% to 3.6%, mirroring human behavior closely.""
  - ""This underscores that for optimal execution of specific professional tasks, incorporating corresponding personality traits in prompts can be an effective strategy when querying the LLM.""",,"
To determine the evaluation metrics used in this paper, I will look for any mentions of specific metrics or measures of performance in the abstract.
""In comparison to the baseline test questions, tasks aligned with the Big Five personality traits manifested superior performance, with enhancements ranging from 0.6% to 3.6%, mirroring human behavior closely.""
This quote indicates that the researchers compared the performance on tasks aligned with personality traits to a baseline, and found enhancements ranging from 0.6% to 3.6%. This suggests that the primary evaluation metric used was some form of performance enhancement or improvement compared to a baseline.
""This underscores that for optimal execution of specific professional tasks, incorporating corresponding personality traits in prompts can be an effective strategy when querying the LLM.""
This quote further reinforces that the researchers were evaluating the performance of the LLM on specific tasks, and found that incorporating personality traits into the prompts led to improved performance.
"
Improving Language Model Prompting in Support of Semi-autonomous Task Learning,"James R. Kirk, R. Wray, Peter Lindes, J. Laird",10.48550/arXiv.2209.07636,https://doi.org/10.48550/arXiv.2209.07636,arXiv.org,10,2022,Prompting strategies can enable language models to provide actionable task knowledge to support agent task learning.,"The paper describes efforts toward a novel agent capability that can construct cues (or ""prompts"") that result in useful language model responses for an agent learning a new task, where the responses must be specific to the agent's task context and in a form that the agent can interpret given its native language capacities.","- The paper found that language models can provide useful knowledge to help agents learn new tasks, but the key is generating responses that are not only reasonable, but also relevant to the agent's specific situation and in a format the agent can understand.
- The paper explored different strategies for constructing prompts to language models to generate useful responses for the agent.","1. Reasonableness - whether the LLM responses would be considered acceptable instructions for the task environment by human experts
2. Situational relevance - how well the LLM responses match the agent's specific embodiment and environmental context
3. Interpretability - whether the agent can parse and ground the LLM responses given its native language capabilities","  - ""that result in useful LLM responses for an agent learning a new task.""
  - ""Language models (LLMs) offer potential as a source of knowledge for agents that need to acquire new task competencies within a performance environment. We describe efforts toward a novel agent capability that can construct cues (or""prompts"")""
  - ""Our results demonstrate that actionable task knowledge can be obtained from LLMs in support of online agent task learning.""
  - ""a measure used commonly in research on knowledge extraction from LLMs) but also specific to the agent's task context and in a form that the agent can interpret given its native language capacities.""
  - ""Importantly, responses must not only be""reasonable""(a""",,"
The paper describes efforts toward a novel agent capability that can construct cues (or ""prompts"") that result in useful language model responses for an agent learning a new task, where the responses must be specific to the agent's task context and in a form that the agent can interpret given its native language capacities.

""Language models (LLMs) offer potential as a source of knowledge for agents that need to acquire new task competencies within a performance environment. We describe efforts toward a novel agent capability that can construct cues (or""prompts"") that result in useful LLM responses for an agent learning a new task.""
""Importantly, responses must not only be""reasonable""(a measure used commonly in research on knowledge extraction from LLMs) but also specific to the agent's task context and in a form that the agent can interpret given its native language capacities.""
""Our results demonstrate that actionable task knowledge can be obtained from LLMs in support of online agent task learning.""


The paper describes efforts toward a novel agent capability that can construct cues (or ""prompts"") that result in useful language model responses for an agent learning a new task, where the responses must be specific to the agent's task context and in a form that the agent can interpret given its native language capacities.


The summary captures the key points of the paper, which are that the researchers are exploring ways for an agent to construct prompts that elicit useful responses from a language model to support the agent's learning of new tasks. The quotes provided fully support this summary by highlighting the focus on constructing prompts, the need for the responses to be relevant to the agent's context and interpretable by the agent, and the overall goal of enabling the agent to learn new tasks.
","  - ""Importantly, responses must not only be""reasonable""(a measure used commonly in research on knowledge extraction from LLMs) but also specific to the agent's task context and in a form that the agent can interpret given its native language capacities.""
  - ""Our results demonstrate that actionable task knowledge can be obtained from LLMs in support of online agent task learning.""
  - ""We describe variations in the construction and parameterization of these prompts and evaluate the responses given those experimental conditions.""",,"
The main findings of this paper can be summarized as follows:


""Our results demonstrate that actionable task knowledge can be obtained from LLMs in support of online agent task learning.""
This indicates that the paper found that language models can provide useful knowledge to help agents learn new tasks.



""Importantly, responses must not only be 'reasonable' (a measure used commonly in research on knowledge extraction from LLMs) but also specific to the agent's task context and in a form that the agent can interpret given its native language capacities.""
This suggests that the key is to generate responses from language models that are not only reasonable, but also relevant to the agent's specific situation and in a format the agent can understand.



""We describe variations in the construction and parameterization of these prompts and evaluate the responses given those experimental conditions.""
The paper explored different strategies for constructing prompts to language models to generate useful responses for the agent.



- The paper found that language models can provide useful knowledge to help agents learn new tasks, but the key is generating responses that are not only reasonable, but also relevant to the agent's specific situation and in a format the agent can understand.
- The paper explored different strategies for constructing prompts to language models to generate useful responses for the agent.



The main findings are well supported by the provided quotes, which cover the key points about obtaining actionable task knowledge from language models, the importance of relevance and interpretability of the responses, and the exploration of different prompt engineering strategies. No additional quotes are needed to fully capture the main findings.
","  - ""A reasonable response is one that humans would generally deem acceptable as an instruction in the task environment.""
  - ""the LLM response must be relevant to the agent's specific situation. This requirement is potentially more challenging than the previous one because, while the statistical patterns of text generation from an LLM can produce reasonable responses, only a small fraction of those responses are likely to be directly relevant to the specific circumstances of the agent.""
  - ""LLMs can generate open-ended, complex natural language utterances that exceed the ability of the agent's parsing and comprehension capabilities. Further, the LLM can introduce terms and concepts that are not known to the agent. Thus, another requirement is that LLM responses are interpretable by the agent.""
  - ""We focus on three: reasonableness, situational relevance, and interpretability. These requirements will form the basis of our evaluation of alternative prompting strategies.""",,"
The paper describes three main evaluation metrics used to assess the performance of the agent's prompting strategies:


""We focus on three: reasonableness, situational relevance, and interpretability. These requirements will form the basis of our evaluation of alternative prompting strategies.""


Reasonableness refers to whether the LLM responses would be considered acceptable instructions for the task environment by human experts. The paper states:


""A reasonable response is one that humans would generally deem acceptable as an instruction in the task environment.""


Situational relevance refers to how well the LLM responses match the agent's specific embodiment and environmental context. The paper explains:


""The LLM response must be relevant to the agent's specific situation. This requirement is potentially more challenging than the previous one because, while the statistical patterns of text generation from an LLM can produce reasonable responses, only a small fraction of those responses are likely to be directly relevant to the specific circumstances of the agent.""


Interpretability refers to whether the agent can parse and ground the LLM responses given its native language capabilities. The paper states:


""LLMs can generate open-ended, complex natural language utterances that exceed the ability of the agent's parsing and comprehension capabilities. Further, the LLM can introduce terms and concepts that are not known to the agent. Thus, another requirement is that LLM responses are interpretable by the agent.""


"
The Tyranny of Possibilities in the Design of Task-Oriented LLM Systems: A Scoping Survey,"Dhruv Dhamani, Mary Lou Maher",10.48550/arXiv.2312.17601,https://doi.org/10.48550/arXiv.2312.17601,arXiv.org,1,2023,The paper explores the design space of task-oriented LLM systems and discusses the impact of prompting techniques on agent behavior.,"The paper presents a scoping survey that explores the design space of task-oriented large language model (LLM) systems, defines key concepts, and proposes several conjectures to guide future research in this area.","- The paper defines a minimal task-oriented LLM system and explores the design space of such systems through a thought experiment, hypothesizing the performance of different LLM system configurations on a software development task.
- The paper formulates three conjectures based on the patterns observed in the thought experiment, acknowledging that these conjectures may be based on faulty assumptions but can serve as a starting point for future research.
- The paper surveys research on LLM augmentation, prompting techniques, and uncertainty estimation, and discusses their significance, while noting the lack of focus on computational and energy efficiency in evaluating this research.",Not mentioned (the abstract does not mention any specific evaluation metrics used in the paper),"  - ""The paper begins by defining a minimal task-oriented LLM system and exploring the design space of such systems through a thought experiment contemplating the performance of diverse LLM system configurations (involving single LLMs, single LLM-based agents, and multiple LLM-based agent systems) on a complex software development task and hypothesizes the results.""
  - ""This scoping survey focuses on our current understanding of the design space for task-oriented LLM systems and elaborates on definitions and relationships among the available design parameters.""
  - ""The paper then surveys a select few design parameters: covering and organizing research in LLM augmentation, prompting techniques, and uncertainty estimation, and discussing their significance.""
  - ""Our survey findings provide a basis for developing the concept of linear and non-linear contexts, which we define and use to enable an agent-centric projection of prompting techniques providing a lens through which prompting techniques can be viewed as multi-agent systems.""",,"
The paper presents a scoping survey that explores the design space of task-oriented large language model (LLM) systems, defines key concepts, and proposes several conjectures to guide future research in this area.
""This scoping survey focuses on our current understanding of the design space for task-oriented LLM systems and elaborates on definitions and relationships among the available design parameters.""
""The paper begins by defining a minimal task-oriented LLM system and exploring the design space of such systems through a thought experiment contemplating the performance of diverse LLM system configurations (involving single LLMs, single LLM-based agents, and multiple LLM-based agent systems) on a complex software development task and hypothesizes the results.""
""The paper then surveys a select few design parameters: covering and organizing research in LLM augmentation, prompting techniques, and uncertainty estimation, and discussing their significance.""
""Our survey findings provide a basis for developing the concept of linear and non-linear contexts, which we define and use to enable an agent-centric projection of prompting techniques providing a lens through which prompting techniques can be viewed as multi-agent systems.""
","  - ""The paper begins by defining a minimal task-oriented LLM system and exploring the design space of such systems through a thought experiment contemplating the performance of diverse LLM system configurations (involving single LLMs, single LLM-based agents, and multiple LLM-based agent systems) on a complex software development task and hypothesizes the results.""
  - ""The paper then surveys a select few design parameters: covering and organizing research in LLM augmentation, prompting techniques, and uncertainty estimation, and discussing their significance. The paper notes the lack of focus on computational and energy efficiency in evaluating research in these areas.""
  - ""We discuss a pattern in our results and formulate them into three conjectures. While these conjectures may be partly based on faulty assumptions, they provide a starting point for future research.""",,"
The main findings of this paper can be summarized as follows:

""The paper begins by defining a minimal task-oriented LLM system and exploring the design space of such systems through a thought experiment contemplating the performance of diverse LLM system configurations (involving single LLMs, single LLM-based agents, and multiple LLM-based agent systems) on a complex software development task and hypothesizes the results.""
The paper defines a minimal task-oriented LLM system and explores the design space of such systems through a thought experiment, hypothesizing the performance of different LLM system configurations on a software development task.

""We discuss a pattern in our results and formulate them into three conjectures. While these conjectures may be partly based on faulty assumptions, they provide a starting point for future research.""
The paper formulates three conjectures based on the patterns observed in the thought experiment, acknowledging that these conjectures may be based on faulty assumptions but can serve as a starting point for future research.

""The paper then surveys a select few design parameters: covering and organizing research in LLM augmentation, prompting techniques, and uncertainty estimation, and discussing their significance. The paper notes the lack of focus on computational and energy efficiency in evaluating research in these areas.""
The paper surveys research on LLM augmentation, prompting techniques, and uncertainty estimation, and discusses their significance, while noting the lack of focus on computational and energy efficiency in evaluating this research.
","  - ""The paper notes the lack of focus on computational and energy efficiency in evaluating research in these areas.""
  - ""The paper begins by defining a minimal task-oriented LLM system and exploring the design space of such systems through a thought experiment contemplating the performance of diverse LLM system configurations (involving single LLMs, single LLM-based agents, and multiple LLM-based agent systems) on a complex software development task and hypothesizes the results.""
  - ""We discuss a pattern in our results and formulate them into three conjectures. While these conjectures may be partly based on faulty assumptions, they provide a starting point for future research.""",,"
The abstract does not explicitly mention any specific evaluation metrics used in the paper. However, it does provide some clues about the types of metrics that may have been used:

""The paper begins by defining a minimal task-oriented LLM system and exploring the design space of such systems through a thought experiment contemplating the performance of diverse LLM system configurations (involving single LLMs, single LLM-based agents, and multiple LLM-based agent systems) on a complex software development task and hypothesizes the results.""
This quote suggests that the paper may have evaluated the performance of different LLM system configurations on a complex software development task, but does not specify the exact metrics used.

""We discuss a pattern in our results and formulate them into three conjectures. While these conjectures may be partly based on faulty assumptions, they provide a starting point for future research.""
This quote indicates that the paper presented some conjectures based on the results, but does not provide details on the specific metrics or data used to derive these conjectures.

""The paper notes the lack of focus on computational and energy efficiency in evaluating research in these areas.""
This quote suggests that the paper may have considered computational and energy efficiency as potential evaluation metrics, but does not state whether these were actually used in the study.
"
LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models,"Ivar Frisch, Mario Giulianelli",10.48550/arXiv.2402.02896,https://doi.org/10.48550/arXiv.2402.02896,PERSONALIZE,3,2024,"The paper examines personality consistency and linguistic alignment in interactions between LLM agents with different personality profiles, but does not measure the impact of system prompts on agent behavior.",This paper presents an experimental study that investigates whether persona-prompted large language models (LLMs) show consistent personality and language use in interaction.,"- Personality-conditioned LLM agents can show consistent personality and language use, but the degree of consistency varies depending on the assigned personality profile.
- While non-interactive language use reflects assigned personality profiles, agents exhibit linguistic alignment towards their conversational partner, with the language of the two agent groups becoming more similar after interaction.
- The degree of linguistic alignment is not symmetric, with the creative persona adapting more towards the analytical one.","1. Responses to the Big Five Inventory (BFI) personality test, to assess the agents' personality consistency before and after the writing tasks.
2. Language use analysis using the Linguistic Inquiry and Word Count (LIWC) software, to quantify differences in language use between the agent groups.
3. Classification accuracy of a logistic regression model trained to distinguish between the language use of the creative and analytical agent groups.","  - ""In brief, our experiments show that consistency to personality profiles varies between agent groups and that linguistic alignment in interaction takes place yet is not symmetric across personas.""
  - ""We bootstrap a population of language agents from a single LLM using a variability-enhancing sampling algorithm, and we condition each agent on a personality profile via natural language prompts. We then simulate interactions between agents and assess their adherence to the specified personality profile-before, during, and after interaction.""
  - ""We condition GPT-3.5 on asymmetric personality profiles to create a population of LLM agents, administer personality tests and submit the agents to a collaborative writing task. We find different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.""",,"
The paper presents an experimental study that investigates whether persona-prompted large language models (LLMs) show consistent personality and language use in interaction. The authors condition GPT-3.5 on two different personality profiles (creative and analytical) and assess the agents' personality consistency and linguistic alignment through personality tests and analysis of language use in individual and collaborative writing tasks.

""We condition GPT-3.5 on asymmetric personality profiles to create a population of LLM agents, administer personality tests and submit the agents to a collaborative writing task. We find different profiles exhibit different degrees of personality consistency and linguistic alignment in interaction.""
""We bootstrap a population of language agents from a single LLM using a variability-enhancing sampling algorithm, and we condition each agent on a personality profile via natural language prompts. We then simulate interactions between agents and assess their adherence to the specified personality profile-before, during, and after interaction.""
""In brief, our experiments show that consistency to personality profiles varies between agent groups and that linguistic alignment in interaction takes place yet is not symmetric across personas.""

","  - ""Furthermore, while non-interactive language use reflects assigned personality profiles, agents exhibit linguistic alignment towards their conversational partner and, as a result, the language of the two agent groups becomes more similar after interaction.""
  - ""The creative persona, in particular, can more consistently express its BFI traits than the analytical one both in the interactive and the non-interactive experimental condition.""
  - ""Alignment, however, is not necessarily symmetric: the creative persona adapts more towards the analytical one, perhaps due to analytical agents' low degree of openness to experience induced through persona prompting.""",,"
The main findings of the paper can be summarized as follows:


1) Personality-conditioned LLM agents can show consistent personality and language use, but the degree of consistency varies depending on the assigned personality profile:
""The creative persona, in particular, can more consistently express its BFI traits than the analytical one both in the interactive and the non-interactive experimental condition.""
2) While non-interactive language use reflects assigned personality profiles, agents exhibit linguistic alignment towards their conversational partner, with the language of the two agent groups becoming more similar after interaction:
""Furthermore, while non-interactive language use reflects assigned personality profiles, agents exhibit linguistic alignment towards their conversational partner and, as a result, the language of the two agent groups becomes more similar after interaction.""
3) The degree of linguistic alignment is not symmetric, with the creative persona adapting more towards the analytical one:
""Alignment, however, is not necessarily symmetric: the creative persona adapts more towards the analytical one, perhaps due to analytical agents' low degree of openness to experience induced through persona prompting.""


The key takeaways are that personality-conditioned LLM agents can show consistent personality and language use, but the degree of consistency varies by profile, and agents exhibit linguistic alignment during interaction, with the creative persona adapting more to the analytical one.
","  - ""In Experiment 1, we test whether personality-conditioned LLM agents show behaviour consistent to their assigned personality profiles, in terms of their responses to personality tests as well as language use in a writing task.""
  - ""In Experiment 2, we assess whether the personality-conditioned behaviour of LLM agents changes as a result of a round of interaction with a conversational partner. This interactive experimental condition allows us to test whether agents' behaviour remains consistent or whether agents align to their partners.""
  - ""A simple logistic regression classifier trained and tested in a 10-fold cross-validation setup on count vectors of LIWC categories obtains an almost perfect average accuracy of 98.5%.""",,"
The paper does not explicitly mention any specific evaluation metrics used to assess the agents' performance or behavior. However, the paper describes two main experiments that were conducted:

""In Experiment 1, we test whether personality-conditioned LLM agents show behaviour consistent to their assigned personality profiles, in terms of their responses to personality tests as well as language use in a writing task.""
This suggests that the agents' personality consistency was evaluated using their responses to personality tests (specifically the Big Five Inventory) and their language use in a writing task.

""In Experiment 2, we assess whether the personality-conditioned behaviour of LLM agents changes as a result of a round of interaction with a conversational partner. This interactive experimental condition allows us to test whether agents' behaviour remains consistent or whether agents align to their partners.""
This indicates that the agents' personality consistency and linguistic alignment were evaluated by comparing their behavior before and after the interactive writing task.

The paper also mentions that a logistic regression classifier was used to distinguish between the language use of the two agent groups:

""A simple logistic regression classifier trained and tested in a 10-fold cross-validation setup on count vectors of LIWC categories obtains an almost perfect average accuracy of 98.5%.""
This suggests that the classification accuracy of the logistic regression model was used as a metric to quantify the differences in language use between the creative and analytical agent groups.
"
Improving Knowledge Extraction from LLMs for Robotic Task Learning through Agent Analysis,"James R. Kirk, R. Wray, Peter Lindes",10.48550/arXiv.2306.06770,https://doi.org/10.48550/arXiv.2306.06770,arXiv.org,4,2023,A cognitive-agent approach that extends prompt engineering can enable robots to acquire new task knowledge from LLMs matched to their capabilities.,"STARS enables an embodied robotic agent to effectively extract and utilize knowledge from a large language model to learn new tasks, addressing the limitations of previous prompt engineering approaches.","- STARS significantly improves task completion rates (77-94%) compared to the baseline TBP approach (35-66%) without user oversight.
- STARS reduces the amount of user oversight required by 52-68% compared to TBP.
- The agent is able to learn tasks in a single shot and then perform them in the future without requiring further interaction with the LLM or the user.","1. Task completion rate
2. Amount of oversight needed to achieve 100% task completion
3. Percentage of accepted goals
4. LLM cost (tokens used for prompts and responses)","  - ""STARS ensures that the goal descriptions it retrieves from the LLM are viable for the agent. Acquiring goal knowledge is crucial to learning novel tasks, enabling an agent with planning capabilities to perform the new task.""
  - ""We describe a cognitive-agent approach that extends and complements prompt engineering, mitigating its limitations, and thus enabling a robot to acquire new task knowledge matched to its native language capabilities, embodiment, environment, and user preferences.""
  - ""STARS reduces user interaction while still ensuring capture of preference. Having the human in the loop also ensures correct learning.""",,"
The paper presents a novel approach called STARS (Search Tree, Analyze and Repair, and Selection) that enables an embodied robotic agent to effectively extract and utilize knowledge from a large language model (LLM) to learn new tasks, addressing the limitations of previous prompt engineering approaches.

""We describe a cognitive-agent approach that extends and complements prompt engineering, mitigating its limitations, and thus enabling a robot to acquire new task knowledge matched to its native language capabilities, embodiment, environment, and user preferences.""
""STARS ensures that the goal descriptions it retrieves from the LLM are viable for the agent. Acquiring goal knowledge is crucial to learning novel tasks, enabling an agent with planning capabilities to perform the new task.""
""STARS reduces user interaction while still ensuring capture of preference. Having the human in the loop also ensures correct learning.""
","  - ""STARS achieves 77-94% task completion without oversight (in comparison to 35-66% with TBP). With oversight, STARS reduces the number of words needed from the user by 52-68% (compared to TBP).""
  - ""because the original ITL agent learns long-term task and subtask knowledge in one shot, this new agent also demonstrates one-shot performance: it achieves 100% task completion when prompted to perform the same task in the future, without accessing the LLM or requiring further human input.""
  - ""providing oversight is much simpler for the user. The user no longer needs to evaluate the viability of responses nor provide (many) goal descriptions; now, the user largely indicates preference, simply confirming or disconfirming from the LLM responses that the agent has determined to be viable.""",,"
The main findings of this paper can be summarized as follows:


""STARS achieves 77-94% task completion without oversight (in comparison to 35-66% with TBP). With oversight, STARS reduces the number of words needed from the user by 52-68% (compared to TBP).""
This indicates that the STARS approach significantly improves task completion rates and reduces the amount of user oversight required compared to the baseline template-based prompting (TBP) approach.



""Providing oversight is much simpler for the user. The user no longer needs to evaluate the viability of responses nor provide (many) goal descriptions; now, the user largely indicates preference, simply confirming or disconfirming from the LLM responses that the agent has determined to be viable.""
This suggests that the STARS approach simplifies the user's role in providing oversight, as they only need to confirm or reject the agent's determined viable responses rather than evaluating and providing goal descriptions.



""Because the original ITL agent learns long-term task and subtask knowledge in one shot, this new agent also demonstrates one-shot performance: it achieves 100% task completion when prompted to perform the same task in the future, without accessing the LLM or requiring further human input.""
This indicates that the agent is able to learn tasks in a single shot and then perform them in the future without requiring further interaction with the LLM or the user.



- STARS significantly improves task completion rates (77-94%) compared to the baseline TBP approach (35-66%) without user oversight.
- STARS reduces the amount of user oversight required by 52-68% compared to TBP.
- The agent is able to learn tasks in a single shot and then perform them in the future without requiring further interaction with the LLM or the user.



The provided quotes fully support the main findings summarized in the bullet points. No additional quotes are needed.
","  - ""We assess both task completion rate and the amount of oversight needed to achieve 100% task completion.""
  - ""For performance, task completion rate (number of goal assertions achieved / total number of goal assertions) is the primary measure.""
  - ""LLM costs are evaluated via tokens presented (prompts) and generated (responses).""
  - ""User effort is the largest factor impacting cost, but cannot be measured directly. To estimate effort, we use the number of interactions and words as well as the percentage of accepted goals.""",,"
The paper discusses several metrics used to evaluate the agent's performance, including:


""We assess both task completion rate and the amount of oversight needed to achieve 100% task completion.""
""For performance, task completion rate (number of goal assertions achieved / total number of goal assertions) is the primary measure.""
""User effort is the largest factor impacting cost, but cannot be measured directly. To estimate effort, we use the number of interactions and words as well as the percentage of accepted goals.""
""LLM costs are evaluated via tokens presented (prompts) and generated (responses).""


The key metrics used are:
1. Task completion rate: The fraction of goal assertions achieved out of the total number of goal assertions.
2. Amount of oversight needed to achieve 100% task completion: Measured by the number of interactions and words from the user.
3. Percentage of accepted goals: The fraction of LLM-generated goals that the user accepts.
4. LLM cost: Measured by the number of tokens used for prompts and responses.

These metrics allow the researchers to evaluate the performance of the agent in terms of task completion, the efficiency of the human-agent interaction, and the computational cost of the approach.
"
Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance,"Ziqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, Satoshi Sekine",10.48550/arXiv.2402.14531,https://doi.org/10.48550/arXiv.2402.14531,Proceedings of the Second Workshop on Social Influence in Conversations (SICon 2024),7,2024,"Politeness levels in prompts impact LLM performance differently across languages, suggesting LLMs reflect human cultural norms.","The paper investigates the impact of prompt politeness on the performance of large language models across different languages, finding that the optimal politeness level varies by language.","- Impolite prompts lead to poor performance in LLMs, suggesting they reflect human desire for respect.
- The optimal level of politeness in prompts varies across languages due to cultural differences.
- Overly polite prompts do not necessarily guarantee better outcomes for LLM performance.","1. For summarization: BERTScore, ROUGE-L, and length (in words for English, characters for Chinese and Japanese)
2. For language understanding benchmarks (MMLU, C-Eval, JMMLU): A modified version of the HELM evaluation method, where an answer is considered correct if the correct choice number appears anywhere in the generated text
3. For stereotypical bias detection: Sentiment analysis on paired bias datasets","  - ""We investigate the impact of politeness levels in prompts on the performance of large language models (LLMs).""
  - ""We observed that impolite prompts often result in poor performance, but overly polite language does not guarantee better outcomes. The best politeness level is different according to the language.""
  - ""This phenomenon suggests that LLMs not only reflect human behavior but are also influenced by language, particularly in different cultural contexts.""",,"
The paper investigates the impact of politeness levels in prompts on the performance of large language models (LLMs) across English, Chinese, and Japanese tasks, finding that impolite prompts often result in poor performance but overly polite language does not guarantee better outcomes, and the best politeness level varies by language.

""We investigate the impact of politeness levels in prompts on the performance of large language models (LLMs).""
""We observed that impolite prompts often result in poor performance, but overly polite language does not guarantee better outcomes. The best politeness level is different according to the language.""
""This phenomenon suggests that LLMs not only reflect human behavior but are also influenced by language, particularly in different cultural contexts.""
","  - ""We observed that impolite prompts often result in poor performance, but excessive flattery is not necessarily welcome, indicating that LLMs reflect the human desire to be respected to a certain extent.""
  - ""the best level of politeness for performance is different across languages, which is strongly related to their cultural background.""
  - ""This finding reveals a deep connection between the behavior of LLMs and human social etiquette""",,"
The main findings of the paper can be summarized as follows:


""We observed that impolite prompts often result in poor performance, but excessive flattery is not necessarily welcome, indicating that LLMs reflect the human desire to be respected to a certain extent.""
""This finding reveals a deep connection between the behavior of LLMs and human social etiquette.""
""The best level of politeness for performance is different across languages, which is strongly related to their cultural background.""


The key takeaways are:
- Impolite prompts lead to poor performance in LLMs, suggesting they reflect human desire for respect.
- The optimal level of politeness in prompts varies across languages due to cultural differences.
- Overly polite prompts do not necessarily guarantee better outcomes for LLM performance.
","  - ""We calculate BERTScore (Zhang et al., 2019) , ROUGE-L (Lin, 2004), and length for all language experiments. The length is counted in words for English and in characters for Chinese and Japanese.""
  - ""for age bias. We conduct sentiment analysis on these pairs to assess positive, neutral, or negative sentiments.""
  - ""We evaluate biases using paired bias datasets, each consisting of two sentences with varying degrees of bias. The sentences are identical apart from bias-specific vocabularies, such as""",,"
The paper discusses several evaluation metrics used to assess the performance of the language models in different tasks:


""We calculate BERTScore (Zhang et al., 2019), ROUGE-L (Lin, 2004), and length for all language experiments. The length is counted in words for English and in characters for Chinese and Japanese.""

The paper uses BERTScore, ROUGE-L, and length as the evaluation metrics for the summarization task.


""For the MMLU, C-Eval, and JMMLU benchmarks, the evaluation method is motivated by HELM (Liang et al., 2023). HELM evaluates based only on the first token of the generated text, considering it incorrect if the LLM does not first answer with the correct choice number. In this study, unlike HELM, an answer is considered correct if the correct choice number appears anywhere in the generated text.""

For the language understanding benchmarks (MMLU, C-Eval, JMMLU), the paper uses a modified version of the HELM evaluation method, where an answer is considered correct if the correct choice number appears anywhere in the generated text, rather than just the first token.


""We evaluate biases using paired bias datasets, each consisting of two sentences with varying degrees of bias. The sentences are identical apart from bias-specific vocabularies, such as ""old"" or ""young"" for age bias. We conduct sentiment analysis on these pairs to assess positive, neutral, or negative sentiments.""

For the stereotypical bias detection, the paper uses paired bias datasets and conducts sentiment analysis to evaluate the models' biases.

"
CogBench: a large language model walks into a psychology lab,"Julian Coda-Forno, Marcel Binz, Jane X. Wang, Eric Schulz",10.48550/arXiv.2402.18225,https://doi.org/10.48550/arXiv.2402.18225,International Conference on Machine Learning,20,2024,"The paper introduces CogBench, a benchmark for evaluating LLM behavior, and finds that prompt engineering techniques like chain-of-thought and take-a-step-back can impact LLM performance on cognitive tasks.",CogBench is a novel benchmark for evaluating the behaviors of large language models based on well-established experiments from cognitive psychology.,"- Larger language models generally perform better and exhibit more model-based behaviors compared to smaller models.
- Reinforcement learning from human feedback (RLHF) makes language models more human-like in their behavior and improves their metacognitive abilities.
- Chain-of-thought prompting enhances probabilistic reasoning, while take-a-step-back prompting promotes more model-based behaviors in language models.","1. Directed and random exploration (Horizon task)
2. Meta-cognition (Restless Bandit task)
3. Learning rate and optimism bias (Instrumental Learning task)
4. Model-basedness (Two-Step task)
5. Temporal discounting (Temporal Discounting task)
6. Risk-taking behavior (Balloon Analog Risk Task)","  - ""CogBench is a benchmark rooted in cognitive psychology for evaluating the behaviors of language models. It incorporates ten metrics derived from seven canonical experiments in the literature on learning and decision-making.""",,"
The paper introduces CogBench, a novel benchmark for evaluating the behaviors of large language models (LLMs) based on well-established experiments from cognitive psychology.

""CogBench is a benchmark rooted in cognitive psychology for evaluating the behaviors of language models. It incorporates ten metrics derived from seven canonical experiments in the literature on learning and decision-making.""
""Our results recover the unequivocal importance of size: larger models generally perform better and are more modelbased than smaller models. Our results also show the importance of reinforcement learning from human feedback (RLHF) in aligning LLMs with humans: RLHF'ed LLMs behave generally more humanlike and are more accurate in estimating uncertainty.""
""Finally, we investigate how chain-of-thought (CoT) and take-a-step-back (SB) prompting techniques can influence different behavioral characteristics. Our analysis suggests that CoT is particularly effective at enhancing probabilistic reasoning, while SB proves to be more relevant for promoting model-based behaviors.""

CogBench is a novel benchmark for evaluating the behaviors of large language models based on well-established experiments from cognitive psychology.

The summary is well supported by the provided quotes, which describe CogBench as a benchmark for evaluating LLM behaviors based on cognitive psychology experiments, and highlight key findings from the paper's analyses. No additional quotes are needed.
",,,"
To summarize the main findings of the paper, I will focus on the key results and conclusions that are most strongly supported by the provided quotes:

Our results recover the unequivocal importance of size: larger models generally perform better and are more modelbased than smaller models.
This quote indicates that the study found model size to be a key factor in improving performance and model-based behavior.

Our results also show the importance of reinforcement learning from human feedback (RLHF) in aligning LLMs with humans: RLHF'ed LLMs behave generally more humanlike and are more accurate in estimating uncertainty.
This quote suggests that the use of RLHF was found to make LLMs more human-like in their behavior and improve their metacognitive abilities.

Finally, we investigate how chain-of-thought (CoT) (Wei et al., 2023;Kojima et al., 2022) and take-a-step-back (SB) (Zheng et al., 2023a) prompting techniques can influence different behavioral characteristics. Our analysis suggests that CoT is particularly effective at enhancing probabilistic reasoning, while SB proves to be more relevant for promoting model-based behaviors.
This quote indicates that the study examined the effects of different prompt engineering techniques, finding that CoT improved probabilistic reasoning while SB enhanced model-based behaviors.
","  - ""We use this task to measure whether an agent uses uncertainty to guide its exploration behavior (directed exploration) and/or whether it injects noise into its policy to explore (random exploration).""
  - ""We use this task to measure meta-cognition, which indicates whether an agent can assess the quality of its own cognitive abilities.""
  - ""We use this task to investigate how an agent learns. First, we report the learning rate of the agent which is common practice in two-armed bandits. Furthermore, we use it to reveal whether an agent learns more from positive than from negative prediction errors, i.e., whether it has an optimism bias.""
  - ""We therefore use it to measure an agent's model-basedness.""
  - ""We use this task to assess temporal discounting, indicating whether an agent prefers smaller but immediate gains over larger delayed ones.""
  - ""We use this task to assess risk-taking behavior.""",,"
The paper introduces the CogBench benchmark, which includes ten behavioral metrics derived from seven cognitive psychology experiments. The specific metrics used to evaluate the agent's performance and behavior are:

""We use this task to measure whether an agent uses uncertainty to guide its exploration behavior (directed exploration) and/or whether it injects noise into its policy to explore (random exploration).""
The Horizon task measures directed and random exploration.

""We use this task to measure meta-cognition, which indicates whether an agent can assess the quality of its own cognitive abilities.""
The Restless Bandit task measures meta-cognition.

""We use this task to investigate how an agent learns. First, we report the learning rate of the agent which is common practice in two-armed bandits. Furthermore, we use it to reveal whether an agent learns more from positive than from negative prediction errors, i.e., whether it has an optimism bias.""
The Instrumental Learning task measures learning rate and optimism bias.

""We therefore use it to measure an agent's model-basedness.""
The Two-Step task measures model-basedness.

""We use this task to assess temporal discounting, indicating whether an agent prefers smaller but immediate gains over larger delayed ones.""
The Temporal Discounting task measures temporal discounting.

""We use this task to assess risk-taking behavior.""
The Balloon Analog Risk Task (BART) measures risk-taking behavior.

"
Evaluating Persona Prompting for Question Answering Tasks,"Carlos Olea, Holly Tucker, Jessica Phelan, Cameron Pattison, Shen Zhang, Maxwell Lieb, Douglas C. Schmidt, Jules White",10.5121/csit.2024.141106,https://doi.org/10.5121/csit.2024.141106,"Security, Privacy and Trust Management",4,2024,The paper evaluates the impact of persona prompting on the performance of large language models in question answering tasks.,"The paper examines the effects of different persona-based prompting strategies, including single-agent and multi-agent personas, on the performance of language models on various question-answering tasks with varying levels of ""openness"" or open-endedness.","- Single-agent expert personas are more effective for higher openness questions compared to other prompting methods.
- Effective prompt engineering is crucial for complex multi-agent methods, which otherwise perform no better than minimal prompting without personas.
- Multi-agent methods require more engineering and processing resources to achieve benefits over simpler prompting approaches.","1. Direct comparison to answer keys for multiple choice questions
2. Semantic similarity scoring by a judge LLM (ChatGPT-4) for short answer questions, with scores of 6 or higher considered correct
3. Numeric question grade average for some datasets, rather than a binary correct/incorrect metric","  - ""This paper further elucidates the effects of personas when using LLMs for question answering tasks. In particular, we measure the performance of various methods of minimal single and multi-agent 1 persona prompting on several datasets of multiple choice and short answer questions.""
  - ""Next, we concluded that for answering multiple choice and short answer questions, as well as advice giving, brainstorming, and other open-ended tasks, there is little-to-no advantage to using the multi-agent roundtable method, particularly with two-to-three experts and one decider LLM to make the final decision.""
  - ""We first concluded that for open-ended, short answer questions there are advantages to using auto-generated (expert) personas versus simply asking the question using a minimal LLM prompt or a minimal persona prompt. This advantage is particularly pronounced in areas like advice giving, brainstorming, and creative writing.""",,"
The paper examines the effects of different persona-based prompting strategies, including single-agent and multi-agent personas, on the performance of language models on various question-answering tasks with varying levels of ""openness"" or open-endedness.

""This paper further elucidates the effects of personas when using LLMs for question answering tasks. In particular, we measure the performance of various methods of minimal single and multi-agent 1 persona prompting on several datasets of multiple choice and short answer questions.""
""We first concluded that for open-ended, short answer questions there are advantages to using auto-generated (expert) personas versus simply asking the question using a minimal LLM prompt or a minimal persona prompt. This advantage is particularly pronounced in areas like advice giving, brainstorming, and creative writing.""
""Next, we concluded that for answering multiple choice and short answer questions, as well as advice giving, brainstorming, and other open-ended tasks, there is little-to-no advantage to using the multi-agent roundtable method, particularly with two-to-three experts and one decider LLM to make the final decision.""

The paper examines the effects of different persona-based prompting strategies, including single-agent and multi-agent personas, on the performance of language models on various question-answering tasks with varying levels of ""openness"" or open-endedness.

The summary is well supported by the provided quotes, which cover the key aspects of the paper's focus and findings. No additional quotes are needed.
","  - ""Single-agent personas are measurably more effective for higher openness questions. Single-agent expert personas correctly answer higher openness questions more often than the other tested methods.""
  - ""Effective prompt engineering becomes more important for complex multi-agent methods. Multi-agent methods implemented without ancillary prompt engineering and ensemble techniques perform no better than minimal prompting with no personas across all levels of openness, thus showing that though multi-agent methods can confer sizeable benefits, they require notably more engineering and processing resources.""
  - ""Single-agent personas are measurably more effective for higher openness questions.""
  - ""Single-agent expert personas correctly answer higher openness questions more often than the other tested methods.""
  - ""effective prompt engineering becomes more important for complex multi-agent methods.""
  - ""Multi-agent methods implemented without ancillary prompt engineering and ensemble techniques perform no better than minimal prompting with no personas across all levels of openness, thus showing that though multi-agent methods can confer sizeable benefits, they require notably more engineering and processing resources.""",,"
The main findings from the paper can be summarized as follows:

""Single-agent personas are measurably more effective for higher openness questions. Single-agent expert personas correctly answer higher openness questions more often than the other tested methods.""
This finding is supported by the paper's observation that ""Single-agent personas are measurably more effective for higher openness questions"" and that ""Single-agent expert personas correctly answer higher openness questions more often than the other tested methods.""

""Effective prompt engineering becomes more important for complex multi-agent methods. Multi-agent methods implemented without ancillary prompt engineering and ensemble techniques perform no better than minimal prompting with no personas across all levels of openness, thus showing that though multi-agent methods can confer sizeable benefits, they require notably more engineering and processing resources.""
The paper concludes that ""Effective prompt engineering becomes more important for complex multi-agent methods"" and that ""Multi-agent methods implemented without ancillary prompt engineering and ensemble techniques perform no better than minimal prompting with no personas across all levels of openness, thus showing that though multi-agent methods can confer sizeable benefits, they require notably more engineering and processing resources.""


- Single-agent expert personas are more effective for higher openness questions compared to other prompting methods.
- Effective prompt engineering is crucial for complex multi-agent methods, which otherwise perform no better than minimal prompting without personas.
- Multi-agent methods require more engineering and processing resources to achieve benefits over simpler prompting approaches.



The key findings are well supported by the provided quotes, which directly state the main conclusions of the paper regarding the effectiveness of single-agent personas for high openness questions and the importance of prompt engineering for complex multi-agent methods. No additional quotes are needed to fully support the summary.
","  - ""For multiple choice questions grading was done by direct comparison to the answer key for a given question provided by a dataset.""
  - ""However, comparisons of judge-compatible datasets were analyzed more by numeric question grade average than by a correct/incorrect method as the multiple choice dataset variants were.""
  - ""For short answer questions, answers were evaluated using a judge LLM, specifically ChatGPT-4.

The judge LLM was instructed to compare the semantic similarity of two statements via the provision of a numeric score from one to ten. It was then given 5 examples of ratings with a static correct answer and varying answers to grade from scores 1-10. Evaluation of short answer questions was performed by providing the LLM judge with the dataset provided correct answer, the generated answer, and the aforementioned examples as part of the system message. The return message from the judge was then parsed for the score, of which scores of 6 or higher were considered ""correct.""""",,"
The paper discusses several evaluation metrics used to assess the performance of the different persona prompting styles:

""For multiple choice questions grading was done by direct comparison to the answer key for a given question provided by a dataset.""
The paper states that for multiple choice questions, the answers were evaluated by directly comparing them to the provided answer key.

""For short answer questions, answers were evaluated using a judge LLM, specifically ChatGPT-4. The judge LLM was instructed to compare the semantic similarity of two statements via the provision of a numeric score from one to ten. It was then given 5 examples of ratings with a static correct answer and varying answers to grade from scores 1-10. Evaluation of short answer questions was performed by providing the LLM judge with the dataset provided correct answer, the generated answer, and the aforementioned examples as part of the system message. The return message from the judge was then parsed for the score, of which scores of 6 or higher were considered ""correct.""""
For short answer questions, the paper states that a judge LLM (ChatGPT-4) was used to evaluate the semantic similarity between the generated answer and the correct answer on a scale of 1-10. Answers with a score of 6 or higher were considered correct.

""However, comparisons of judge-compatible datasets were analyzed more by numeric question grade average than by a correct/incorrect method as the multiple choice dataset variants were.""
The paper also mentions that for some datasets, the numeric grade average was used for analysis rather than a binary correct/incorrect metric.
"